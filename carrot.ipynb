{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e6fb38",
   "metadata": {},
   "source": [
    "# Carrot Catcher\n",
    "#### This program takes footage from a camera, filters orange objects from each frame, feeds these filtered frames to the carrot identifying model, and then saves the model output to a MySQL database.\n",
    "\n",
    "Throughout this assessment, I made a few assumptions, since we were supposed to assume that a model was already given to us. I will give explanations for these assumptions along with my thought process for the functions I implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79808954",
   "metadata": {},
   "source": [
    "---\n",
    "## Libraries\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecf0f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import mysql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d22847",
   "metadata": {},
   "source": [
    "---\n",
    "## Set Up Camera and Model\n",
    "---\n",
    "\n",
    "#### Video Capture\n",
    "Camera footage is collected live by using OpenCV's VideoCapture method. The default input, 0, just gets the default camera from your computer. If we were actually using this in a grocery store, then we would have to give the program the grocery store's camera path.\n",
    "\n",
    "#### Model Set Up\n",
    "The loading of ML models usually depends on a loading method from the library that was used to make the model. For this assessment, let's just assume that the model is a TensorFlow model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ea11e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "                                                                                                                              \n",
    "# Load ML model\n",
    "modelPath = \"Insert Model Path Here\"\n",
    "model = tf.keras.models.load_model(modelPath) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce234b2",
   "metadata": {},
   "source": [
    "---\n",
    "## Set Up Data Base\n",
    "---\n",
    "\n",
    "#### Database Decisions\n",
    "As you can see, this isn't a real database. If we were to implement a real database, then we would obviously have to give real parameters. For our implmementation, I used MySQL to record carrot IDs from the model output. For a more sophisticated implementation, adding more output data would be ideal. For example, it might be a good idea to include details such as size of carrot, date of recording, and carrot location in the frame. Additionally, it could also be better to use Postegres instead of MySQL, so that we could use array types for location/coordinate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102259bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MySQL DB\n",
    "db = mysql.connector.connect(\n",
    "    user=\"user\",\n",
    "    password=\"password\",\n",
    "    host=\"host IP\",\n",
    "    database=\"database name\"\n",
    ")\n",
    "\n",
    "cursor = db.cursor()\n",
    "\n",
    "# Prepare DB insertions\n",
    "query = (\"INSERT IGNORE INTO carrots (carrot_id) \" # IGNORE catches duplicates\n",
    "        \"VALUES (%s)\")\n",
    "\n",
    "def insertCarrot(carrotID):\n",
    "    cursor.execute(query, carrotID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636621b6",
   "metadata": {},
   "source": [
    "---\n",
    "## Image Cleaning\n",
    "---\n",
    "#### Why Filter The Image?\n",
    "Some models can simply be given unfiltered images and categorize the objects in the frame. I initially went with this assumption, but I felt like I wasn't really showing my knowledge on categorization by only feeding a raw image to the model. Therefore, I decided to filter the orange colours from the image as a way to somewhat isolate the carrots from the rest of the image. \n",
    "\n",
    "#### Notes on Sources\n",
    "I followed the general process for colour masking using [this video](https://www.youtube.com/watch?v=aFNDh5k3SjU). This involves taking the HSV (Hue, Saturation, and Value) measurements from the image and isolating the values that are in the HSV range of orange. I determined the range by following [this Stack Overflow post](https://stackoverflow.com/questions/10948589/choosing-the-correct-upper-and-lower-hsv-boundaries-for-color-detection-withcv). To be honest, colour isolation is pretty finicky, so my implementation is definitely not optimal. It would be ideal to run an algorithm to determine values based on each picture, rather than a blanket range for all oranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd38b02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask for orange using the HSV values of the image\n",
    "def filterOrange(img):\n",
    "    hsvImg = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    lowerLim = (12, 100, 100)\n",
    "    upperLim = (25, 255, 255)\n",
    "    mask = cv2.inRange(hsvImg, lowerLim, upperLim)\n",
    "    newImg = cv2.bitwise_and(img, img ,mask= mask)\n",
    "    return newImg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd131386",
   "metadata": {},
   "source": [
    "---\n",
    "## Image Detection\n",
    "---\n",
    "\n",
    "### Output Assumptions\n",
    "Models can output any kind of data. For my implementation, I just assumed that the model returned an array of IDs for each carrot found in the image. \n",
    "\n",
    "### General Program Layout\n",
    "This kind of while loop is pretty common in OpenCV projects that analyze video data. As you can see, we check to see if the camera/video is working, clean the data, then put that image data into a model. Afterwards, we record the output and display the image in some way. For the purposes of this assessment, it wasn't necessary to display the image of the camera or to include a way to close the program, but I still included them, since they would be useful in a real implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723b7f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Checks if camera/video is working\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    if not img:\n",
    "        print(\"No image could be found from your camera\")\n",
    "        break\n",
    "\n",
    "    # Feed image to the model\n",
    "    cleanedImg = filterOrange(img)\n",
    "    results = model(cleanedImg) # Returns an id for each unique carrot found\n",
    "\n",
    "    # Add each carrot to the DB\n",
    "    for carrot in results:\n",
    "        insertCarrot(carrotID=carrot)\n",
    "\n",
    "    # Show camera footage\n",
    "    cv2.imshow(\"Image\", img)\n",
    "     \n",
    "    # Press ESC to close program\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f17da4a",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Thoughts\n",
    "---\n",
    "This little assessment was kind of fun. It reminded me of a different OpenCV project that I did last year. That one involved identifying the shape of guitar chords using a TensorFlow model. You can check out that project on [my repo here](https://github.com/benkyli/Guitar-Chord-Identifier)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
